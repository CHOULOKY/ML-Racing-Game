Lambda: 업데이트된 가치를 예측할 때 현재 예측된 가치에 얼마나 의존할지 결정. 이 값이 낮으면 현재 예측된 가치에 더 의존하는 것을 의미하며(높은 편향 (bias) 발생 가능), 값이 높으면 환경을 통해 얻은 실제 보상에 더 의존하는 것을 의미(높은 분산 (variance) 발생 가능)
(0.9 - 0.95)

Buffer Size: 모델 학습을 시작하기 전 얼마나 많은 경험들(관측, 행동, 보상 등)을 저장할지 결정. 이 값은 batch_size의 배수로 설정되어야
일반적으로 큰 buffer_size는 더 안정적인 학습을 가능하게
(2048 - 409600)

Batch Size: 한번의 경사하강(Gradient Descent) 업데이트를 수행할 때 사용할 경험들의 수를 의미. 이 값은 항상 buffer_size의 약수로 설정되어야.
만약 연속적인 행동 공간(Continuous Action Space) 환경을 사용하는 경우 이 값은 크게 설정되어야(1000의 단위). 만약 이산적인 행동 공간(Discrete Action Space) 환경을 사용하는 경우 이 값은 더 작게 설정되어야(10의 단위).
연속적인 행동 (512 - 5120)
이산적인 행동 (32 - 512)

Number of Epochs: 경사 하강(Gradient Descent) 학습 동안 경험 버퍼(Experience Buffer) 데이터에 대해 학습을 몇번 수행할 지 결정
batch_size가 클수록 이 값도 커져야. 이 값을 줄이면 더 안정적인 업데이트가 보장되지만 학습 속도가 느려집
(3 - 10)

Learning Rate: 경사 하강(Gradient Descent) 학습의 정도를 결정
학습이 불안정하고 에이전트가 얻는 보상이 증가하지 않는 경우 일반적으로 학습률을 감소
(1e-5 - 1e-3)

Time Horizon: 경험 버퍼(Experience Buffer)에 저장하기 전 에이전트당 수집할 경험의 스텝 수
한 에피소드 동안 보상이 빈번하게 발생하는 경우나 에피소드가 엄청나게 긴 경우에는 time horizon 값은 작게 설정하는 것이 이상적
(32 - 2048)

Beta: 정책을 더 랜덤하게 만들 수. 이 값을 통해 에이전트는 학습 동안 액션 공간을 적절하게 탐험할 수. 이 값을 증가시키면 에이전트가 더 많이 랜덤 행동을, 엔트로피(텐서보드를 통해 측정 가능)는 보상이 증가함에 따라 서서히 크기를 감소시켜야.
만약 엔트로피가 너무 빠르게 떨어지면 beta를 증가시켜야. 만약 엔트로피가 너무 느리게 떨어지면 beta를 감소시켜야.
(1e-4 - 1e-2)

Epsilon: 작게 설정되면 더 안정적인 학습이 가능하지만 학습이 느리게 진행될 것
(0.1 - 0.3)

Normalize: 정규화는 복잡하고 연속적인 제어 문제에서 도움이 될 수 있지만, 단순하고 이산적인 제어 문제에서는 정규화를 사용하는 것이 좋지 않을 수

Number of Layers: 간단한 문제에서는 적은 수의 층을 사용하여 빠르고 효율적으로 학습, 복잡한 제어 문제에서는 많은 층을 사용 필요
(1 - 3)

Hidden Units: 최적의 행동이 관측 입력의 간단한 조합으로 결정되는 단순한 문제 작게, 최적의 행동이 관측 입력의 복잡한 관계에 의해 결정되는 어려운 문제 크게
(32 - 512)
